# Benchmark Results Directory

This directory contains benchmark test results and reports generated by the OpenRouter MCP Server benchmarking system.

## Directory Structure

```
benchmarks/
├── *.json           # Raw benchmark results in JSON format
├── *.csv            # Benchmark reports in CSV format
├── *.md             # Benchmark reports in Markdown format
└── README.md        # This file
```

## File Naming Convention

- **Benchmark Results**: `benchmark_YYYYMMDD_HHMMSS.json`
- **Test Benchmarks**: `test_benchmark_YYYYMMDD_HHMMSS.json`
- **CSV Reports**: `test_report_YYYYMMDD_HHMMSS.csv`
- **Markdown Reports**: `test_report_YYYYMMDD_HHMMSS.md`

## Usage

### Generating New Benchmarks

Use the benchmark tools to create new benchmark results:

```python
from src.openrouter_mcp.handlers.mcp_benchmark import benchmark_models

results = await benchmark_models(
    models=["openai/gpt-4", "anthropic/claude-3"],
    prompt="Your test prompt",
    runs=3
)
```

### Viewing Results

- **JSON files**: Raw benchmark data for programmatic analysis
- **CSV files**: Import into spreadsheet applications for analysis
- **MD files**: Human-readable reports with formatted tables

### Cleaning Up Old Results

To remove old benchmark files, you can use the cleanup script:

```python
from src.openrouter_mcp.handlers.benchmark_cleanup import cleanup_old_benchmarks

# Remove files older than 30 days
cleanup_old_benchmarks(days_to_keep=30)
```

## Data Privacy

⚠️ **Note**: Benchmark files may contain API responses and prompts. Do not commit sensitive data to version control.

## See Also

- [Benchmarking Guide](../docs/BENCHMARK_GUIDE.md) - Complete benchmarking documentation
- [API Documentation](../docs/API.md) - API reference for benchmark tools

---

**Last Updated**: 2025-01-12