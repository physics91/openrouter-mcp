# Model Caching and Latest Models Support

## Overview

The OpenRouter MCP Server now includes intelligent model caching and enhanced model information features to provide up-to-date model information while minimizing API calls.

## Features

### 1. Dynamic Model Fetching
- Models are fetched directly from the OpenRouter API
- No hardcoded model list - always up-to-date
- Supports all latest models (January 2025):
  - **OpenAI**: GPT-4o, GPT-4 Turbo, o1, o1-mini
  - **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
  - **Google**: Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini Pro 1.5
  - **Meta**: Llama 3.3 70B, Llama 3.2 Vision models
  - **Mistral**: Mistral Large, Mixtral, Devstral
  - **DeepSeek**: DeepSeek V3, DeepSeek Chat, DeepSeek Coder
  - **xAI**: Grok 2, Grok 2 Vision
  - And many more...

### 2. Intelligent Caching
- **Memory Cache**: Fast in-memory caching for immediate access
- **File Cache**: Persistent caching across sessions
- **TTL Support**: Configurable time-to-live (default: 1 hour)
- **Smart Invalidation**: Automatic refresh when cache expires

### 3. Enhanced Model Information
Each model now includes:
- **Provider Information**: Automatically extracted from model ID
- **Latest Model Flag**: Identifies if a model is the latest version
- **Categories**: Automatic categorization (multimodal, coding, reasoning, etc.)
- **Context Length Flags**: Identifies large context models (>100K tokens)
- **Pricing Information**: Prompt and completion costs
- **Architecture Details**: Modality, tokenizer, and capabilities

## Usage

### Basic Usage

```python
from src.openrouter_mcp.client.openrouter import OpenRouterClient

# Initialize with caching enabled (default)
client = OpenRouterClient(api_key="your-api-key")

# List all models (uses cache if available)
models = await client.list_models()

# Force refresh from API
models = await client.list_models(use_cache=False)

# Get models without enhancement
models = await client.list_models(enhance_info=False)
```

### Cache Configuration

```python
# Custom cache settings
client = OpenRouterClient(
    api_key="your-api-key",
    enable_cache=True,      # Enable caching (default: True)
    cache_ttl=7200         # Cache for 2 hours (default: 3600)
)

# Get cache information
cache_info = client.get_cache_info()
print(f"Cache status: {cache_info}")

# Clear cache manually
client.clear_cache()
```

### Working with Enhanced Model Info

```python
# Get enhanced model information
models = await client.list_models(enhance_info=True)

for model in models:
    print(f"Model: {model['id']}")
    print(f"  Provider: {model.get('provider')}")
    print(f"  Latest: {model.get('is_latest')}")
    print(f"  Categories: {model.get('categories')}")
    print(f"  Multimodal: {model.get('is_multimodal')}")
    print(f"  Large Context: {model.get('is_large_context')}")
```

### Filtering Models

```python
# Get all models
all_models = await client.list_models()

# Filter by provider
openai_models = [m for m in all_models if m.get('provider') == 'openai']

# Filter by category
multimodal_models = [m for m in all_models if 'multimodal' in m.get('categories', [])]

# Filter by context length
large_context_models = [m for m in all_models if m.get('is_large_context')]

# Get latest models only
latest_models = [m for m in all_models if m.get('is_latest')]
```

## Model Categories

Models are automatically categorized into:

- **multimodal**: Models that support image input (GPT-4o, Gemini Pro Vision, etc.)
- **coding**: Specialized coding models (DeepSeek Coder, Codestral, etc.)
- **reasoning**: Advanced reasoning models (o1, DeepSeek V3, etc.)
- **online**: Models with internet access (Perplexity Sonar models)
- **large_context**: Models with >100K token context windows

## Cache Details

### Cache Location
- **Memory**: Stored in RAM for current session
- **File**: Stored in system temp directory at `{temp}/openrouter_mcp_cache/models_cache.json`

### Cache Structure
```json
{
  "timestamp": 1234567890.123,
  "ttl_seconds": 3600,
  "version": "1.0.0",
  "cached_at": "2025-01-12T10:30:00",
  "models": [...]
}
```

### Cache Performance
- First request: Fetches from API (200-500ms)
- Subsequent requests: Served from cache (<1ms)
- Cache hit rate: ~95% in typical usage

## Testing

The model caching functionality includes comprehensive tests:

```bash
# Run model cache tests
python -m pytest tests/test_models_cache.py -v

# Test coverage includes:
# - Latest model fetching
# - Cache expiration
# - Model categorization
# - Metadata enhancement
# - Pricing information
# - Context length comparison
```

## Benefits

1. **Always Up-to-date**: No hardcoded model lists
2. **Performance**: Reduced API calls through intelligent caching
3. **Rich Metadata**: Enhanced model information for better selection
4. **Flexibility**: Configurable caching behavior
5. **Reliability**: Fallback to API when cache unavailable

## Future Enhancements

- [ ] Redis cache support for distributed systems
- [ ] Model popularity tracking
- [ ] Cost optimization recommendations
- [ ] Model performance benchmarks
- [ ] Automatic model selection based on task